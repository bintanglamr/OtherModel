# -*- coding: utf-8 -*-
"""MODEL COMPARISON [7].ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hozuTYBNOZDmTmdVBDk-OVifpoQhsx3m

# Install Libraries
"""

import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (mean_squared_error, r2_score,
                             mean_absolute_error, explained_variance_score,
                             max_error)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
plt.rcParams["figure.figsize"] = (12,5)
import warnings
warnings.filterwarnings('ignore')

"""# Load Data"""

# Load and preprocess the dataset
df = pd.read_csv("/content/busan_dataset.csv")
df.columns = df.columns.str.strip()  # Remove spaces from column names
df['Date'] = pd.to_datetime(df['Date'])
df.set_index('Date', inplace=True)

"""# Data Exploration"""

# Display basic information and statistics
print(df.shape, df.info(), df.describe(), df.isna().sum(), sep='\n')

# Feature Engineering
df['hour'] = df.index.hour
df['month'] = df.index.month
required_cols = ['GHI_Average', 'SunZenith_KMU', 'Ambient_Pressure',
                 'Water', 'AOD', 'Uo (atm-cm)', 'CI_Hammer', 'OT', 'hour', 'month']
df = df[required_cols]

df.head()

# Correlation Matrix
correlation_matrix = df.corr()
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='seismic', linewidths=0.75, fmt=".2f")
plt.title('Correlation Matrix of Input Variables')
plt.show()

"""# Data Splitting & Scaling"""

# Split the dataset
train_ratio, test_ratio, validation_ratio = 0.7, 0.15, 0.15
train_size = int(len(df) * train_ratio)
test_size = int(len(df) * test_ratio)
train, test, validation = df.iloc[:train_size], df.iloc[train_size:train_size + test_size], df.iloc[train_size + test_size:]

# Input Scaling
cols = ['SunZenith_KMU', 'Ambient_Pressure', 'Water', 'AOD',
        'OT', 'Uo (atm-cm)', 'CI_Hammer', 'hour', 'month']

scaler = RobustScaler()
train[cols] = scaler.fit_transform(train[cols])
test[cols] = scaler.transform(test[cols])

# Scaling GHI
GHI_scaler = RobustScaler()
GHI_scaler.fit(train[['GHI_Average']])
train['GHI_Average'] = GHI_scaler.transform(train[['GHI_Average']])
test['GHI_Average'] = GHI_scaler.transform(test[['GHI_Average']])

print(f'Train size: {len(train)}, Test size: {len(test)}, Validation size: {len(validation)}')

"""# Datasets Building

# Create Dataset
"""

# ---------- 3 DIMENSIONAL DATASET CREATION (For LSTM, GRU, RNN) ----------
def create_3d_dataset(X, y, time_steps=7, horizon=1):
    """
    Creates 3-dimensional dataset for training/testing with a specified number of previous timesteps including the current one, and horizon.
    This is suitable for models like LSTM, GRU, and RNN.
    """
    Xs, ys = [], []
    for i in range(len(X) - time_steps - horizon + 1):
        Xs.append(X.iloc[i:i + time_steps].values)  # Include the current timestep in the last position of the window
        ys.append(y.iloc[i + time_steps + horizon - 1])  # Target variable at the specified horizon
    return np.array(Xs), np.array(ys)

# ---------- 2 DIMENSIONAL DATASET CREATION (For ANN, SVR, MLP, DNN) ----------
def create_2d_dataset(X, y, time_steps=1, horizon=1):
    """
    Creates 2-dimensional dataset for training/testing with a specified horizon.
    This is suitable for models like ANN, SVR, MLP, and DNN.
    """
    Xs, ys = [], []
    for i in range(len(X) - time_steps - horizon + 1):
        Xs.append(X.iloc[i + time_steps - 1].values)  # Previous time step features
        ys.append(y.iloc[i + time_steps + horizon - 1])  # Target variable at the specified horizon
    return np.array(Xs), np.array(ys)

"""# Forecasting Horizon"""

# ---------- SET FORECASTING HORIZONS ----------
horizons = [1, 2, 3]  # 1-hour, 2-hour, and 3-hour ahead

"""# Define Dataset"""

# ---------- CREATE DATASETS FOR 3D MODELS (LSTM, GRU, RNN) ----------
datasets_3d = {}
for h in horizons:
    X_train_3d, y_train_3d = create_3d_dataset(train, train['GHI_Average'], time_steps=7, horizon=h)
    X_test_3d, y_test_3d = create_3d_dataset(test, test['GHI_Average'], time_steps=7, horizon=h)
    datasets_3d[h] = (X_train_3d, y_train_3d, X_test_3d, y_test_3d)

# ---------- CREATE DATASETS FOR 2D MODELS (c) ----------
datasets_2d = {}
for h in horizons:
    X_train_2d, y_train_2d = create_2d_dataset(train, train['GHI_Average'], time_steps=1, horizon=h)
    X_test_2d, y_test_2d = create_2d_dataset(test, test['GHI_Average'], time_steps=1, horizon=h)
    datasets_2d[h] = (X_train_2d, y_train_2d, X_test_2d, y_test_2d)

print("3D Datasets (For LSTM, GRU, RNN):")
for horizon, data in datasets_3d.items():
    print(f"Horizon {horizon}-hour ahead: X_train shape: {data[0].shape}, y_train shape: {data[1].shape}")

print("\n2D Datasets (For ANN, SVR, MLP, DNN):")
for horizon, data in datasets_2d.items():
    print(f"Horizon {horizon}-hour ahead: X_train shape: {data[0].shape}, y_train shape: {data[1].shape}")

"""# Model Building"""

# Initialize dictionaries to store results and predictions for all models
results_all = {}
predictions_all = {}
histories_all = {}

# Define a function to calculate accuracy metrics
def calculate_metrics(y_true, y_pred):
    """Calculate and return model performance metrics."""
    metrics = {
        'R^2': r2_score(y_true, y_pred),
        'MAE': mean_absolute_error(y_true, y_pred),
        'MSE': mean_squared_error(y_true, y_pred),
        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),
    }
    return metrics



# Define a common function for training neural network models
def train_and_evaluate_nn(model_name, build_model_fn, datasets, epochs=100, batch_size=64):
    results_model = {}
    predictions_model = {}
    histories_model = {}

    for h in horizons:
        X_train, y_train = datasets[h][:2]  # Get the training data
        X_test, y_test = datasets[h][2:]    # Get the testing data

        # Build the model using the provided function
        model = build_model_fn(X_train.shape)

        # Compile and train the model
        history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.15, shuffle=False)
        histories_model[h] = history

        # Predictions
        y_pred = model.predict(X_test)
        y_pred_inv = GHI_scaler.inverse_transform(y_pred.reshape(-1, 1))
        y_test_inv = GHI_scaler.inverse_transform(y_test.reshape(-1, 1))

        # Store predictions
        predictions_model[h] = (y_test_inv.flatten(), y_pred_inv.flatten())

        # Calculate and store metrics
        metrics = calculate_metrics(y_test_inv, y_pred_inv)
        results_model[h] = metrics

    results_all[model_name] = results_model
    predictions_all[model_name] = predictions_model
    histories_all[model_name] = histories_model

import matplotlib.pyplot as plt

# Function to plot training and validation loss for all horizons side by side
def plot_model_loss_all_horizons(histories, model_name, horizons):
    num_horizons = len(horizons)

    # Create subplots
    fig, axes = plt.subplots(1, num_horizons, figsize=(5 * num_horizons, 5), sharey=True)
    fig.suptitle(f'Model Loss for {model_name} across All Horizons', fontsize=16)

    for idx, h in enumerate(horizons):
        # Extract the training and validation loss
        history = histories[h]
        loss = history.history['loss']
        val_loss = history.history['val_loss']

        # Plot on the respective subplot
        axes[idx].plot(loss, label='Training Loss')
        axes[idx].plot(val_loss, label='Validation Loss')
        axes[idx].set_title(f'{h} Hour Ahead')
        axes[idx].set_xlabel('Epochs')
        axes[idx].set_ylabel('Loss')
        axes[idx].legend()

    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit the title
    plt.show()

"""# GRU"""

def build_gru(input_shape):
    model = tf.keras.Sequential([
        tf.keras.layers.GRU(units=100, activation='relu', input_shape=(input_shape[1], input_shape[2])),
        tf.keras.layers.Dropout(rate=0.5),
        tf.keras.layers.Dense(units=1)
    ])
    model.compile(loss='mean_squared_error', optimizer='adam')
    return model

train_and_evaluate_nn('GRU', build_gru, datasets_3d)

# Visualize the loss for the GRU model across all horizons
plot_model_loss_all_horizons(histories_all['GRU'], 'GRU', horizons)

"""# ANN"""

# Define individual model architectures
def build_ann(input_shape):
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(units=100, activation='relu', input_shape=(input_shape[1],)),
        tf.keras.layers.Dropout(rate=0.5),
        tf.keras.layers.Dense(units=1)
    ])
    model.compile(loss='mean_squared_error', optimizer='adam')
    return model

train_and_evaluate_nn('ANN', build_ann, datasets_2d)

# Visualize the loss for the GRU model across all horizons
plot_model_loss_all_horizons(histories_all['ANN'], 'ANN', horizons)

"""# LSTM"""

def build_lstm(input_shape):
    model = tf.keras.Sequential([
        tf.keras.layers.LSTM(units=100, activation='relu', input_shape=(input_shape[1], input_shape[2])),
        tf.keras.layers.Dropout(rate=0.5),
        tf.keras.layers.Dense(units=1)
    ])
    model.compile(loss='mean_squared_error', optimizer='adam')
    return model

train_and_evaluate_nn('LSTM', build_lstm, datasets_3d)

# Visualize the loss for the GRU model across all horizons
plot_model_loss_all_horizons(histories_all['LSTM'], 'LSTM', horizons)

"""# RNN"""

def build_rnn(input_shape):
    model = tf.keras.Sequential([
        tf.keras.layers.SimpleRNN(units=100, activation='relu', input_shape=(input_shape[1], input_shape[2])),
        tf.keras.layers.Dropout(rate=0.5),
        tf.keras.layers.Dense(units=1)
    ])
    model.compile(loss='mean_squared_error', optimizer='adam')
    return model

train_and_evaluate_nn('RNN', build_rnn, datasets_3d)

# Visualize the loss for the GRU model across all horizons
plot_model_loss_all_horizons(histories_all['RNN'], 'RNN', horizons)

"""# DNN"""

def build_dnn(input_shape):
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(units=100, activation='relu', input_shape=(input_shape[1],)),
        tf.keras.layers.Dropout(rate=0.5),
        tf.keras.layers.Dense(units=100, activation='relu'),
        tf.keras.layers.Dropout(rate=0.5),
        tf.keras.layers.Dense(units=50, activation='relu'),
        tf.keras.layers.Dense(units=1)
    ])
    model.compile(loss='mean_squared_error', optimizer='adam')
    return model

train_and_evaluate_nn('DNN', build_dnn, datasets_2d)

# Visualize the loss for the GRU model across all horizons
plot_model_loss_all_horizons(histories_all['DNN'], 'DNN', horizons)

"""# TRANSFORMER"""

def build_transformer(input_shape, num_heads=4, dff=128, d_model=64, dropout_rate=0.1):
    # Input layer
    inputs = tf.keras.Input(shape=(input_shape[1], input_shape[2]))

    # Transformer encoder block
    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs)
    attention_output = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)
    attention_output = tf.keras.layers.Dropout(dropout_rate)(attention_output)
    x = tf.keras.layers.Add()([inputs, attention_output])

    # Feed-forward network
    ffn_output = tf.keras.layers.Dense(dff, activation='relu')(x)
    ffn_output = tf.keras.layers.Dense(input_shape[2])(ffn_output)
    ffn_output = tf.keras.layers.Dropout(dropout_rate)(ffn_output)
    x = tf.keras.layers.Add()([x, ffn_output])

    # Global average pooling layer
    x = tf.keras.layers.GlobalAveragePooling1D()(x)

    # Dense output layer
    outputs = tf.keras.layers.Dense(1)(x)

    # Create the model
    model = tf.keras.Model(inputs=inputs, outputs=outputs)

    # Compile the model
    model.compile(optimizer='adam', loss='mean_squared_error')

    return model

# Train and evaluate the Transformer model with 3D datasets
train_and_evaluate_nn('Transformer', build_transformer, datasets_3d)

# Visualize the loss for the GRU model across all horizons
plot_model_loss_all_horizons(histories_all['Transformer'], 'Transformer', horizons)

"""# SVR"""

from sklearn.svm import SVR
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# SVR Model training and evaluation separately using 2D datasets
results_svr = {}
predictions_svr = {}

for h in horizons:
    X_train, y_train = datasets_2d[h][:2]
    X_test, y_test = datasets_2d[h][2:]

    # Train the SVR model
    svr_model = SVR()
    svr_model.fit(X_train, y_train)

    # Predictions
    y_pred = svr_model.predict(X_test)
    y_pred_inv = GHI_scaler.inverse_transform(y_pred.reshape(-1, 1))
    y_test_inv = GHI_scaler.inverse_transform(y_test.reshape(-1, 1))

    # Store predictions
    predictions_svr[h] = (y_test_inv.flatten(), y_pred_inv.flatten())

    # Calculate metrics
    metrics = calculate_metrics(y_test_inv, y_pred_inv)
    results_svr[h] = metrics

# Store SVR results
results_all['SVR'] = results_svr
predictions_all['SVR'] = predictions_svr

"""# Visualization"""

# Visualization and comparison
for model_name in results_all:
    print(f"Results for {model_name}:")
    for h in horizons:
        print(f"Horizon {h}: {results_all[model_name][h]}")

# Prepare data for plotting
model_names = list(results_all.keys())
horizons = [1, 2, 3]  # Example horizons for 1, 2, and 3 hours
rmse_values = {model: [] for model in model_names}

# Extract RMSE values from results_all for each model and horizon
for model in model_names:
    for h in horizons:
        rmse = results_all[model][h]['RMSE']
        rmse_values[model].append(rmse)

# Set up the horizontal bar graph
bar_width = 0.25  # Width of each bar
y_pos = np.arange(len(model_names))  # Positions for the model names

fig, ax = plt.subplots(figsize=(14, 10))

# Define colors for each horizon
colors = ['#ffd900', '#e81980', '#0096e0']

# Create horizontal bars for each model and horizon
for i, h in enumerate(horizons):
    # Position bars with an offset for each horizon
    y_offset = y_pos + (i * bar_width)  # Offset for each horizon
    bars = ax.barh(y_offset, [rmse_values[model][i] for model in model_names],
                    bar_width, label=f'{h} hour', color=colors[i])

    # Add labels on the right side of each bar
    for bar in bars:
        ax.text(bar.get_width(), bar.get_y() + bar.get_height() / 2,
                f'{bar.get_width():.2f}',
                ha='left', va='center')

# Customize y-ticks for model names
ax.set_yticks(y_pos + bar_width)
ax.set_yticklabels(model_names)

# Set the legend below the plot
ax.legend(title='Forecasting Horizons', bbox_to_anchor=(0.5, -0.1), loc='upper center', ncol=3)

# Add labels and title
ax.set_xlabel('RMSE Values')
ax.set_title('RMSE Comparison of Models Across Forecasting Horizons')

# Show the plot
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Number of points to visualize
num_points = 50

# Assuming the first horizon corresponds to one-hour ahead
horizon_one_hour = horizons[0]  # This should be set to the correct index if needed

# Extract actual data from the DataFrame's GHI_Average column
actual_data = y_test_inv.flatten()[-num_points:]  # Last 100 actual data points

# Create a new figure for the plot
plt.figure(figsize=(14, 8))

# Plot actual data
plt.plot(actual_data, label='Actual Data', color='black', linewidth=2)

# Plot predictions for each model for the one-hour horizon
for model_name, predictions in predictions_all.items():
    if horizon_one_hour in predictions:
        y_pred_inv = predictions[horizon_one_hour][1]  # Get inverse-transformed predictions
        pred_data = y_pred_inv[-num_points:]  # Get the last 100 points of predictions
        plt.plot(pred_data, label=f'{model_name} Prediction - Horizon {horizon_one_hour} hours', linestyle='--')

# Adding titles and labels
plt.title('Comparison of One-Hour Ahead Forecasting by Different Models')
plt.xlabel('Time Steps')
plt.ylabel('Global Horizontal Irradiance (GHI)')
plt.legend()
plt.grid(True)

# Show the plot
plt.show()

# Number of points to visualize
num_points = 50

# Assuming the second horizon corresponds to two-hours ahead
horizon_two_hours = horizons[1]  # Change the index to match two-hours ahead

# Extract actual data from the DataFrame's GHI_Average column
actual_data = y_test_inv.flatten()[-num_points:]  # Last 100 actual data points

# Create a new figure for the plot
plt.figure(figsize=(14, 8))

# Plot actual data
plt.plot(actual_data, label='Actual Data', color='black', linewidth=2)

# Plot predictions for each model for the two-hour horizon
for model_name, predictions in predictions_all.items():
    if horizon_two_hours in predictions:
        y_pred_inv = predictions[horizon_two_hours][1]  # Get inverse-transformed predictions
        pred_data = y_pred_inv[-num_points:]  # Get the last 100 points of predictions
        plt.plot(pred_data, label=f'{model_name} Prediction - Horizon {horizon_two_hours} hours', linestyle='--')

# Adding titles and labels
plt.title('Comparison of Two-Hour Ahead Forecasting by Different Models')
plt.xlabel('Time Steps')
plt.ylabel('Global Horizontal Irradiance (GHI)')
plt.legend()
plt.grid(True)

# Show the plot
plt.show()

import matplotlib.pyplot as plt

# Number of points to visualize
num_points = 50

# Assuming the third horizon corresponds to three-hours ahead
horizon_three_hours = horizons[2]  # Change the index to match three-hours ahead

# Extract actual data from the DataFrame's GHI_Average column
actual_data = y_test_inv.flatten()[-num_points:]  # Last 100 actual data points

# Create a new figure for the plot
plt.figure(figsize=(14, 8))

# Plot actual data
plt.plot(actual_data, label='Actual Data', color='black', linewidth=2)

# Plot predictions for each model for the three-hour horizon
for model_name, predictions in predictions_all.items():
    if horizon_three_hours in predictions:
        y_pred_inv = predictions[horizon_three_hours][1]  # Get inverse-transformed predictions
        pred_data = y_pred_inv[-num_points:]  # Get the last 100 points of predictions
        plt.plot(pred_data, label=f'{model_name} Prediction - Horizon {horizon_three_hours} hours', linestyle='--')

# Adding titles and labels
plt.title('Comparison of Three-Hour Ahead Forecasting by Different Models')
plt.xlabel('Time Steps')
plt.ylabel('Global Horizontal Irradiance (GHI)')
plt.legend()
plt.grid(True)

# Show the plot
plt.show()

# Prepare data for regression plots
regression_data = []

# Extract actual data from the predictions for each model
for model_name, predictions in predictions_all.items():
    if 1 in predictions:  # Check for one-hour horizon
        y_pred_inv = predictions[1][1]  # Inverse transformed predictions
        y_test_inv = predictions[1][0]   # Inverse transformed actual data
        # Create a DataFrame for the model's predictions
        model_data = pd.DataFrame({
            'Actual': y_test_inv,
            'Predicted': y_pred_inv.flatten()
        })
        model_data['Model'] = model_name
        regression_data.append(model_data)

# Concatenate all model data into a single DataFrame
regression_data = pd.concat(regression_data, ignore_index=True)

# Set up the matplotlib figure
plt.figure(figsize=(20, 15))

# Create regression plots for each model
for model_name in regression_data['Model'].unique():
    model_subset = regression_data[regression_data['Model'] == model_name]

    # Calculate R² value
    r2 = r2_score(model_subset['Actual'], model_subset['Predicted'])

    plt.subplot(4, 2, regression_data['Model'].unique().tolist().index(model_name) + 1)
    sns.regplot(x='Actual', y='Predicted', data=model_subset, scatter_kws={'alpha': 0.6}, line_kws={"color": "red"})
    plt.title(f'Regression Plot for {model_name} - One Hour Ahead Forecasting')
    plt.xlabel('Actual)')
    plt.ylabel('Predicted')

    # Annotate the R² value on the plot
    plt.text(0.05, 0.95, f'$R^2 = {r2:.2f}$', fontsize=12, transform=plt.gca().transAxes,
             verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', edgecolor='none', facecolor='lightgrey'))

    plt.grid(True)

# Adjust layout
plt.tight_layout()
plt.show()

# Prepare data for regression plots
regression_data = []

# Extract actual data from the predictions for each model
for model_name, predictions in predictions_all.items():
    if 1 in predictions:  # Check for one-hour horizon
        y_pred_inv = predictions[2][1]  # Inverse transformed predictions
        y_test_inv = predictions[2][0]   # Inverse transformed actual data
        # Create a DataFrame for the model's predictions
        model_data = pd.DataFrame({
            'Actual': y_test_inv,
            'Predicted': y_pred_inv.flatten()
        })
        model_data['Model'] = model_name
        regression_data.append(model_data)

# Concatenate all model data into a single DataFrame
regression_data = pd.concat(regression_data, ignore_index=True)

# Set up the matplotlib figure
plt.figure(figsize=(20, 15))

# Create regression plots for each model
for model_name in regression_data['Model'].unique():
    model_subset = regression_data[regression_data['Model'] == model_name]

    # Calculate R² value
    r2 = r2_score(model_subset['Actual'], model_subset['Predicted'])

    plt.subplot(4, 2, regression_data['Model'].unique().tolist().index(model_name) + 1)
    sns.regplot(x='Actual', y='Predicted', data=model_subset, scatter_kws={'alpha': 0.6}, line_kws={"color": "red"})
    plt.title(f'Regression Plot for {model_name} - Two Hour Ahead Forecasting')
    plt.xlabel('Actual)')
    plt.ylabel('Predicted')

    # Annotate the R² value on the plot
    plt.text(0.05, 0.95, f'$R^2 = {r2:.2f}$', fontsize=12, transform=plt.gca().transAxes,
             verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', edgecolor='none', facecolor='lightgrey'))

    plt.grid(True)

# Adjust layout
plt.tight_layout()
plt.show()

# Prepare data for regression plots
regression_data = []

# Extract actual data from the predictions for each model
for model_name, predictions in predictions_all.items():
    if 3 in predictions:  # Check for three-hour horizon
        y_pred_inv = predictions[3][1]  # Inverse transformed predictions for 3-hour
        y_test_inv = predictions[3][0]   # Inverse transformed actual data for 3-hour
        # Create a DataFrame for the model's predictions
        model_data = pd.DataFrame({
            'Actual': y_test_inv,
            'Predicted': y_pred_inv.flatten()
        })
        model_data['Model'] = model_name
        regression_data.append(model_data)

# Concatenate all model data into a single DataFrame
regression_data = pd.concat(regression_data, ignore_index=True)

# Set up the matplotlib figure
plt.figure(figsize=(20, 15))

# Create regression plots for each model
for model_name in regression_data['Model'].unique():
    model_subset = regression_data[regression_data['Model'] == model_name]

    # Calculate R² value
    r2 = r2_score(model_subset['Actual'], model_subset['Predicted'])

    plt.subplot(4, 2, regression_data['Model'].unique().tolist().index(model_name) + 1)
    sns.regplot(x='Actual', y='Predicted', data=model_subset, scatter_kws={'alpha': 0.6}, line_kws={"color": "red"})
    plt.title(f'Regression Plot for {model_name} - Three Hour Ahead Forecasting')
    plt.xlabel('Actual')
    plt.ylabel('Predicted')

    # Annotate the R² value on the plot
    plt.text(0.05, 0.95, f'$R^2 = {r2:.2f}$', fontsize=12, transform=plt.gca().transAxes,
             verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', edgecolor='none', facecolor='lightgrey'))

    plt.grid(True)

# Adjust layout
plt.tight_layout()
plt.show()