# -*- coding: utf-8 -*-
"""Transformer-BO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RK0HA3s_tWTqyu2HXmqnYcDo6FgZ2WK_
"""

!pip install optuna

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, r2_score
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt
import seaborn as sns
import optuna
from optuna import Trial
from itertools import combinations
import os

from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression

# Specify the output directory (adjust the path as needed)
output_directory = '/content/'  # Replace with your desired path
if not os.path.exists(output_directory):
    os.makedirs(output_directory)  # Create the directory if it doesn't exist

Station = 'BJSR'

# Load and preprocess your dataset from Excel
file_path = f'/content/{Station}_2years.xlsx'  # Update with your local file path
df = pd.read_excel(file_path, sheet_name='Sheet1',
                   usecols=['WIB','sr_avg','tt_air_max' , 'tt_air_avg', 'Sun_Zenith_Angle'
                            ,'wd_avg','rh_avg' ])

# Filter rows where 'Sun_Zenith_Angle' is below 80
df = df[df['Sun_Zenith_Angle'] < 80]

# Ensure that 'time' column is the index or remove if unnecessary
df['WIB'] = pd.to_datetime(df['WIB'])
df.set_index('WIB', inplace=True)

# Feature Engineering
df['hour'] = df.index.hour
df['month'] = df.index.month
df['DOY'] = df.index.day_of_year


# Show Table Info
df.info()

df.tail()

# Correlation Matrix
correlation_matrix = df.corr()
plt.figure(figsize=(10, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='inferno', linewidths=0.1, fmt=".1f")
plt.title('Correlation MaQtrix of Input Variables')
plt.show()

"""# **Model Building**"""

def prepare_data(selected_features, sequence_length=7, horizon=3):
    scaler_X = MinMaxScaler()
    scaler_y = MinMaxScaler()

    input_columns = ['sr_avg'] + selected_features
    scaled_X = scaler_X.fit_transform(df[input_columns].values)
    scaled_y = scaler_y.fit_transform(df[['sr_avg']].values)

    X, y = [], []
    for i in range(len(scaled_X) - sequence_length - horizon + 1):
        X.append(scaled_X[i:i + sequence_length])
        y.append(scaled_y[i + sequence_length + horizon - 1, 0])

    X, y = np.array(X), np.array(y)
    train_size = int(len(X) * 0.8)
    return X[:train_size], X[train_size:], y[:train_size], y[train_size:], scaler_X, scaler_y

def transformer_model(input_shape, embedding_dim, num_heads, ff_dim, d_model, dropout_rate):
    inputs = Input(shape=input_shape)
    x = Dense(embedding_dim)(inputs)  # Project input to embedding_dim
    for _ in range(3):  # Use 3 Transformer blocks
        attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)
        attn_output = Dropout(dropout_rate)(attn_output)  # Apply dropout
        x = LayerNormalization(epsilon=1e-6)(x + attn_output)

        # Align x's last dimension with d_model for residual connection
        ffn_output = Dense(ff_dim, activation='relu')(x)
        ffn_output = Dense(d_model)(ffn_output)
        ffn_output = Dropout(dropout_rate)(ffn_output)  # Apply dropout
        x = LayerNormalization(epsilon=1e-6)(Dense(d_model)(x) + ffn_output)

    x = GlobalAveragePooling1D()(x)
    outputs = Dense(1)(x)
    return Model(inputs, outputs)

def objective(trial):
    selected_features = trial.suggest_categorical('selected_features', feature_combinations)
    selected_features = list(selected_features)

    embedding_dim = trial.suggest_int('embedding_dim', 32, 128, step=16)
    num_heads = trial.suggest_int('num_heads', 2, 8, step=2)
    ff_dim = trial.suggest_int('ff_dim', 32, 128, step=16)
    d_model = trial.suggest_int('d_model', 32, 128, step=16)
    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5, step=0.1)  # Suggest dropout rate
    epochs = trial.suggest_int('epochs', 10, 100, step=10)
    batch_size = trial.suggest_categorical('batch_size', [32, 64])

    X_train, X_test, y_train, y_test, _, _ = prepare_data(selected_features, horizon=3)

    model = transformer_model(X_train.shape[1:], embedding_dim, num_heads, ff_dim, d_model, dropout_rate)
    model.compile(optimizer=tf.keras.optimizers.Adam(), loss='mean_squared_error')

    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    history = model.fit(X_train, y_train, validation_data=(X_test, y_test),
                        epochs=epochs, batch_size=batch_size, verbose=0, callbacks=[early_stopping])

    # Plot training and validation loss
    plt.figure(figsize=(10, 6))
    plt.plot(history.history['loss'], label='Training Loss', color='blue')
    if 'val_loss' in history.history:
        plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.show()

    y_pred = model.predict(X_test)
    return mean_squared_error(y_test, y_pred)

all_features = ['tt_air_max', 'tt_air_avg', 'Sun_Zenith_Angle', 'wd_avg', 'rh_avg', 'hour', 'month', 'DOY']
feature_combinations = [list(comb) for i in range(1, len(all_features) + 1) for comb in combinations(all_features, i)]

study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=5)

best_params = study.best_params
best_features = best_params.pop('selected_features')

print(f"Best features: {best_features}")
print(f"Best hyperparameters: {best_params}")

final_model = transformer_model(
    X_train.shape[1:],
    best_params['embedding_dim'],
    best_params['num_heads'],
    best_params['ff_dim'],
    best_params['d_model'],
    best_params['dropout_rate']  # Include dropout_rate
)

final_model.compile(optimizer=tf.keras.optimizers.Adam(), loss='mean_squared_error')
history = final_model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=best_params['epochs'],
    batch_size=best_params['batch_size'],
    verbose=1
)

# Plot training and validation loss
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Training Loss', color='blue')
if 'val_loss' in history.history:
    plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

y_pred = final_model.predict(X_test)
y_test_inv = scaler_y.inverse_transform(y_test.reshape(-1, 1))
y_pred_inv = scaler_y.inverse_transform(y_pred)

plt.figure(figsize=(16, 6))
plt.plot(y_test_inv, label='Actual')
plt.plot(y_pred_inv, label='Predicted', linestyle='--')
plt.legend()
plt.show()

# Linear regression scatter plot with R² value
regressor = LinearRegression()
regressor.fit(y_test_inv, y_pred_inv)
y_pred_lr = regressor.predict(y_test_inv)

plt.figure(figsize=(8, 6))
plt.scatter(y_test_inv, y_pred_inv, color='blue', alpha=0.5)
plt.plot(y_test_inv, y_pred_lr, color='red', linewidth=2)
r2 = r2_score(y_test_inv, y_pred_inv)
plt.title(f'Linear Regression: R² = {r2:.2f}')
plt.xlabel('Actual Solar Radiation')
plt.ylabel('Predicted Solar Radiation')
plt.show()

"""# **Validation**"""

# Create the result DataFrame with shifted WIB
result_df = pd.DataFrame({
    'WIB': pd.Series(df.index[-len(y_test_inv):]).shift(0),  # Shifted Time steps (WIB)
    'Actual_SR': y_test_inv.flatten(),
    'Predicted_SR_Transformer': y_pred_inv.flatten()
})


# Assuming result_df contains 'Actual_SR' and 'Predicted_SR'
actual = result_df['Actual_SR'].values
predicted = result_df['Predicted_SR_Transformer'].values

# Calculate RMSE
rmse = np.sqrt(mean_squared_error(actual, predicted))

# Calculate rRMSE (Relative RMSE as a percentage)
mean_actual = np.mean(actual)
rrmse = (rmse / mean_actual) * 100

# Calculate MBE
mbe = np.mean(predicted - actual)

# Calculate rMBE (Relative MBE as a percentage)
rmbe = (mbe / mean_actual) * 100

# Print the metrics
print(f"RMSE: {rmse:.4f}")
print(f"rRMSE: {rrmse:.2f}%")
print(f"MBE: {mbe:.4f}")
print(f"rMBE: {rmbe:.2f}%")

result_df.tail(15)

"""# **Export**"""

# Dynamically set the file name
results_table = f'Transformer-BO_{Station}_3HA(2).xlsx'

# Combine directory and file name
output_path = os.path.join(output_directory, results_table)

# Export the DataFrame to an Excel file
result_df.to_excel(output_path, index=False)

print(f"File saved at: {output_path}")

# Define all possible features
all_possible_features = [
    'tt_air_max', 'tt_air_avg', 'Sun_Zenith_Angle', 'wd_avg', 'ws_avg',
    'ws_max', 'rh_avg', 'hour', 'month', 'DOY'
]

# Create a DataFrame for features with boolean indicators
features_df = pd.DataFrame({
    'Feature': all_possible_features,
    'Selected': [feature in best_features for feature in all_possible_features]
})

# Map boolean values to true/false
features_df['Selected'] = features_df['Selected'].map({True: '=TRUE', False: '=FALSE'})

# Convert features_df into a DataFrame that contains Parameter and Value columns
features_df = features_df.rename(columns={'Feature': 'Parameter', 'Selected': 'Value'})

# Convert best_params dictionary to DataFrame
params_df = pd.DataFrame(list(best_params.items()), columns=['Parameter', 'Value'])

# Combine both DataFrames
combined_table = pd.concat([params_df, features_df], ignore_index=True)

# Display the combined table
print("Best Parameters and Features Table:")
print(combined_table)

# Dynamically set the file name
params_table = f'BestParams_Transformer-BO_{Station}_3HA(2)_.xlsx'

# Combine directory and file name to form the full path
output_path = os.path.join(output_directory, params_table)

# Export the DataFrame to an Excel file
combined_table.to_excel(output_path, index=False)

print(f"File saved at: {output_path}")